version: '3.8'

services:
  postgres: #Airflow's metadata store
    image: postgres:13 
    container_name: airflow-postgres
    environment:
      POSTGRES_USER: ${AIRFLOW_POSTGRES_USER}
      POSTGRES_PASSWORD: ${AIRFLOW_POSTGRES_PASSWORD}
      POSTGRES_DB: ${AIRFLOW_POSTGRES_DB}
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    ports:
      - "5432:5432" #PostgreSQL port 5432, allowing external connections if needed

  airflow-webserver: #provides the web-based user interface for managing and monitoring workflows
    image: apache/airflow:3.0.0
    container_name: airflow-webserver
    depends_on:
      - postgres
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor #tasks run locally within the same machine rather than being distributed across multiple workers
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${AIRFLOW_POSTGRES_USER}:${AIRFLOW_POSTGRES_PASSWORD}@postgres/${AIRFLOW_POSTGRES_DB}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY}
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      _PIP_ADDITIONAL_REQUIREMENTS: 'python-dotenv pymongo' #official Airflow mechanism that automatically installs the specified packages on container startup, caches the packages in the Docker volume (airflow-data). Is much faster on subsequent container restarts
      # Airflow admin credentials from .env
      _AIRFLOW_WWW_USER_USERNAME: ${AIRFLOW_ADMIN_USERNAME:-admin}
      _AIRFLOW_WWW_USER_PASSWORD: ${AIRFLOW_ADMIN_PASSWORD:-admin}
      # Environment variables for your project
      MONGO_ATLAS_URI: ${MONGO_ATLAS_URI}
    volumes:
      - airflow-data:/opt/airflow
      - ./airflow/dags:/opt/airflow/dags
      # - ./airflow/logs:/opt/airflow/logs
      - ./.env:/opt/airflow/.env
    ports:
      - "8080:8080"
    command: >
      bash -c "airflow db migrate &&
               airflow users create --username ${AIRFLOW_ADMIN_USERNAME} --password ${AIRFLOW_ADMIN_PASSWORD} --firstname Admin --lastname User --role Admin --email admin@example.com || true &&
               airflow webserver"
  airflow-scheduler: #responsible for parsing DAG files, determining which tasks need to run, and triggering their execution. The scheduler continuously monitors your DAGs and schedules tasks based on their dependencies and timing requirements.
    image: apache/airflow:3.0.0
    container_name: airflow-scheduler
    depends_on:
      - postgres
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${AIRFLOW_POSTGRES_USER}:${AIRFLOW_POSTGRES_PASSWORD}@postgres/${AIRFLOW_POSTGRES_DB}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY}
      _PIP_ADDITIONAL_REQUIREMENTS: 'python-dotenv pymongo'
      # Environment variables for your project
      MONGO_ATLAS_URI: ${MONGO_ATLAS_URI}
    volumes:
      - airflow-data:/opt/airflow
      - ./airflow/dags:/opt/airflow/dags
      # - ./airflow/logs:/opt/airflow/logs
      - ./.env:/opt/airflow/.env
    command: airflow scheduler

volumes:
  postgres-db-volume:
  airflow-data:
