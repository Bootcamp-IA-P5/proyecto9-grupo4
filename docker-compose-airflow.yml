version: '3.8'

services:
  postgres: #Airflow's metadata store
    image: postgres:13 
    container_name: airflow-postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    ports:
      - "5432:5432" #PostgreSQL port 5432, allowing external connections if needed

  airflow-webserver: #provides the web-based user interface for managing and monitoring workflows
    image: apache/airflow:3.0.0
    container_name: airflow-webserver
    depends_on:
      - postgres
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor #tasks run locally within the same machine rather than being distributed across multiple workers
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY}
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      _PIP_ADDITIONAL_REQUIREMENTS: 'python-dotenv pymongo' #official Airflow mechanism that automatically installs the specified packages on container startup, caches the packages in the Docker volume (airflow-data). Is much faster on subsequent container restarts
      # Airflow admin credentials from .env
      _AIRFLOW_WWW_USER_USERNAME: ${AIRFLOW_ADMIN_USERNAME:-admin}
      _AIRFLOW_WWW_USER_PASSWORD: ${AIRFLOW_ADMIN_PASSWORD:-admin}
      # Environment variables for your project
      MONGO_ATLAS_URI: ${MONGO_ATLAS_URI}
    volumes:
      - airflow-data:/opt/airflow
      - ./airflow/dags:/opt/airflow/dags
      # - ./airflow/logs:/opt/airflow/logs
      - ./.env:/opt/airflow/.env
    ports:
      - "8080:8080"
    command: >
      bash -c "airflow db migrate &&
               airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com || true &&
               airflow webserver"
  airflow-scheduler: #responsible for parsing DAG files, determining which tasks need to run, and triggering their execution. The scheduler continuously monitors your DAGs and schedules tasks based on their dependencies and timing requirements.
    image: apache/airflow:3.0.0
    container_name: airflow-scheduler
    depends_on:
      - postgres
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY}
      _PIP_ADDITIONAL_REQUIREMENTS: 'python-dotenv pymongo'
      # Environment variables for your project
      MONGO_ATLAS_URI: ${MONGO_ATLAS_URI}
    volumes:
      - airflow-data:/opt/airflow
      - ./airflow/dags:/opt/airflow/dags
      # - ./airflow/logs:/opt/airflow/logs
      - ./.env:/opt/airflow/.env
    command: airflow scheduler

volumes:
  postgres-db-volume:
  airflow-data:
