# Project Goals

The primary goal of this project is to implement a robust and scalable data pipeline that processes data in real-time. The pipeline demonstrates the integration of several key technologies:

-   **Data Ingestion**: Consume streaming data from a **Kafka** topic with 5 different schemas.
-   **Data Staging**: Store the raw, unstructured data in a **MongoDB** collection, which acts as a flexible staging area.
-   **Data Consolidation**: Merge the 5 disparate schemas into unified "golden records" representing complete individual profiles.
-   **Data Processing & Storage**: Transform the consolidated data from MongoDB and load it into a structured **PostgreSQL** (Supabase) relational database for querying and analysis.

This project serves as a practical example of building an ETL (Extract, Transform, Load) process suitable for modern data engineering challenges, all within a containerized development environment with Apache Airflow orchestration.

The project management board can be found [here](https://github.com/orgs/Bootcamp-IA-P5/projects/16).

## Pipeline Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Kafka     â”‚  5 schemas: Personal, Location, Professional, Bank, Network
â”‚  (5 schemas)â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ read_from_kafka.py
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MongoDB                â”‚
â”‚  probando_messages      â”‚  â† Raw, unconsolidated Kafka messages
â”‚  (staging)              â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ consolidate_mongodb_records.py
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MongoDB                â”‚
â”‚  golden_records         â”‚  â† Consolidated person-level records
â”‚  (consolidated)         â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ mongodb_to_supabase.py
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Supabase (PostgreSQL)  â”‚
â”‚  - person               â”‚
â”‚  - bank                 â”‚  â† Normalized relational model
â”‚  - work                 â”‚
â”‚  - address              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

# Getting Started

To get the development environment up and running, follow these steps. For more detailed contribution guidelines, see [`CONTRIBUTING.md`](./CONTRIBUTING.md).

### Prerequisites

- **Docker Desktop**: The project uses VS Code Dev Containers.
- **VS Code**: With the Dev Containers extension installed.
- **Running Services**: You need access to running instances of Kafka, MongoDB, and PostgreSQL.

### Setup

1.  **Clone the repository**:
    ```sh
    git clone https://github.com/Bootcamp-IA-P5/proyecto9-grupo4.git
    cd proyecto9-grupo4
    ```
2.  **Configure Environment**: Copy the `.env.example` file to `.env` and add your credentials for Kafka and PostgreSQL.
3.  **Launch Dev Container**: Open the project folder in VS Code and run the command `Dev Containers: Reopen in Container`. This will build the Docker container and install all dependencies from `requirements.txt`.

# Project structure

```
proyecto9-grupo4/
â”œâ”€â”€ airflow/                       # Apache Airflow for pipeline orchestration
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ kafka_mongodb_observer.py      # DAG for monitoring Kafkaâ†’MongoDB pipeline
â”‚   â”‚   â””â”€â”€ complete_etl_pipeline.py       # ğŸ†• Full ETL orchestration DAG
â”‚   â”œâ”€â”€ README.md                  # Airflow setup and usage guide
â”‚   â””â”€â”€ setup_airflow.sh          # Installation script
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â””â”€â”€ logger.py              # Logging configuration and utilities
â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â””â”€â”€ sql.py             # SQLAlchemy ORM models
â”‚   â”‚   â”œâ”€â”€ sql_alchemy.py         # ğŸ”„ Enhanced: PostgreSQL connection & ETL functions
â”‚   â”‚   â”œâ”€â”€ write_to_mongodb.py    # Functions to connect and write to MongoDB
â”‚   â”‚   â””â”€â”€ check_mongodb.py       # MongoDB inspection utilities
â”‚   â””â”€â”€ __init__.py                # Makes 'src' a Python package
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ settings.py                # Environment/configuration variables
â”‚   â””â”€â”€ kafka.ini                  # Kafka-specific configuration
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ test_data.json         # Sample golden records for testing
â”‚   â””â”€â”€ test_consolidation.py      # ğŸ†• Tests for consolidation logic
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ read_from_kafka.py         # 1ï¸âƒ£ Reads from Kafka â†’ MongoDB (raw)
â”‚   â”œâ”€â”€ consolidate_mongodb_records.py  # ğŸ†• 2ï¸âƒ£ Consolidates 5 schemas â†’ golden records
â”‚   â”œâ”€â”€ mongodb_to_supabase.py     # ğŸ†• 3ï¸âƒ£ Loads golden records â†’ Supabase
â”‚   â”œâ”€â”€ CONSOLIDATION_README.md    # ğŸ†• Detailed consolidation documentation
â”‚   â”œâ”€â”€ sql_load_db.py             # Utility to load test data into SQL database
â”‚   â”œâ”€â”€ sql_clean_db.py            # Utility to clean and recreate SQL database
â”‚   â””â”€â”€ sql_dump_db.py             # Utility to export SQL data to CSV
â”œâ”€â”€ .devcontainer/
â”‚   â””â”€â”€ devcontainer.json          # VSCode Dev Container configuration
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ ISSUE_TEMPLATE/            # GitHub issue templates
â”œâ”€â”€ requirements.txt               # List of project dependencies
â”œâ”€â”€ .env.example                   # Example environment variables file
â””â”€â”€ README.md                      # Project documentation
```

ğŸ†• = Newly added components for data consolidation

# Usage

This section describes how to run the various scripts provided in the project.

## Complete ETL Pipeline

### Option 1: Automated with Airflow (Recommended)

The complete pipeline is orchestrated by the `complete_etl_pipeline` DAG:

```sh
# 1. Start Airflow (if not already running)
cd airflow
./setup_airflow.sh

# 2. Access Airflow UI: http://localhost:8080 (admin/admin)
# 3. Enable and trigger the 'complete_etl_pipeline' DAG
```

The DAG runs hourly and executes:
1. âœ… Check for new raw Kafka messages
2. ğŸ”„ Consolidate records (5 schemas â†’ golden records)
3. ğŸ“¤ Load golden records â†’ Supabase
4. âœ”ï¸ Validate data integrity

### Option 2: Manual Execution

Run each stage of the pipeline manually:

**Stage 1: Kafka â†’ MongoDB (Raw)**
```sh
python -m scripts.read_from_kafka
```
This continuously reads Kafka messages and stores them in `kafka_data.probando_messages`.

**Stage 2: MongoDB Raw â†’ Golden Records**
```sh
# Production run
python scripts/consolidate_mongodb_records.py

# Dry run (test without writing)
python scripts/consolidate_mongodb_records.py --dry-run
```
This consolidates the 5 schemas into unified golden records in `kafka_data.golden_records`.

**Stage 3: MongoDB Golden â†’ Supabase**
```sh
python scripts/mongodb_to_supabase.py
```
This loads golden records into PostgreSQL tables (person, bank, work, address).

## Testing

Test the consolidation logic with sample data:
```sh
python tests/test_consolidation.py
```

## Database Utilities

**Load test data into PostgreSQL:**
```sh
python -m scripts.sql_load_db [-h|--file FILE]
```

**Clean the PostgreSQL database:**
```sh
python -m scripts.sql_clean_db [-h|-f|--force]
```

**Dump PostgreSQL database to CSV:**
```sh
python -m scripts.sql_dump_db [-h|[-o|--output] OUTPUT]
```

## Monitoring & Observability

### Airflow DAGs

1. **`kafka_mongodb_health_monitor`** - Monitors stage 1 (Kafka â†’ MongoDB)
   - Checks MongoDB connectivity
   - Validates data freshness
   - Tracks insertion rates

2. **`complete_etl_pipeline`** - Orchestrates full pipeline
   - Runs consolidation
   - Loads to Supabase
   - Validates data integrity

Access Airflow UI: http://localhost:8080 (admin/admin)

### Manual Monitoring

**Check MongoDB collections:**
```sh
python src/database/check_mongodb.py
```

**Check Supabase tables:**
```sql
-- In Supabase SQL Editor
SELECT 
  (SELECT COUNT(*) FROM person) as persons,
  (SELECT COUNT(*) FROM bank) as banks,
  (SELECT COUNT(*) FROM work) as work,
  (SELECT COUNT(*) FROM address) as addresses;
```
